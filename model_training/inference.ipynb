{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultralytics\n",
    "import cv2\n",
    "import cv2.dnn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.98 ðŸš€ Python-3.12.3 torch-2.4.1+cu121 CPU (AMD Ryzen 5 PRO 3400G with Radeon Vega Graphics)\n",
      "Model summary (fused): 168 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8_supernew.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (6.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.5s, saved as 'yolov8_supernew.onnx' (11.7 MB)\n",
      "\n",
      "Export complete (1.0s)\n",
      "Results saved to \u001b[1m/home/taras-desktop/Documents/ucu/embeddedAI/project/model_training\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov8_supernew.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=yolov8_supernew.onnx imgsz=640 data=/content//data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8_supernew.onnx'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ultralytics.YOLO('yolov8n.pt')\n",
    "# export to onnx\n",
    "model.export(format='onnx', opset=12)  # You can adjust the opset if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 288x416 8 Trees, 159.4ms\n",
      "Speed: 10.2ms preprocess, 159.4ms inference, 0.6ms postprocess per image at shape (1, 3, 288, 416)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9737])\n",
      "data: tensor([[540.8509, 274.1178, 691.3217, 453.2866,   0.9737,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[616.0863, 363.7022, 150.4708, 179.1688]])\n",
      "xywhn: tensor([[0.8521, 0.7546, 0.2081, 0.3717]])\n",
      "xyxy: tensor([[540.8509, 274.1178, 691.3217, 453.2866]])\n",
      "xyxyn: tensor([[0.7481, 0.5687, 0.9562, 0.9404]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9711])\n",
      "data: tensor([[ 42.5557, 271.5143, 214.1543, 461.3437,   0.9711,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[128.3550, 366.4290, 171.5986, 189.8293]])\n",
      "xywhn: tensor([[0.1775, 0.7602, 0.2373, 0.3938]])\n",
      "xyxy: tensor([[ 42.5557, 271.5143, 214.1543, 461.3437]])\n",
      "xyxyn: tensor([[0.0589, 0.5633, 0.2962, 0.9571]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9680])\n",
      "data: tensor([[534.4786,  79.5344, 686.5923, 234.0956,   0.9680,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[610.5355, 156.8150, 152.1136, 154.5612]])\n",
      "xywhn: tensor([[0.8444, 0.3253, 0.2104, 0.3207]])\n",
      "xyxy: tensor([[534.4786,  79.5344, 686.5923, 234.0956]])\n",
      "xyxyn: tensor([[0.7393, 0.1650, 0.9496, 0.4857]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9628])\n",
      "data: tensor([[355.9091,  77.0469, 526.3854, 236.8754,   0.9628,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[441.1473, 156.9611, 170.4762, 159.8284]])\n",
      "xywhn: tensor([[0.6102, 0.3256, 0.2358, 0.3316]])\n",
      "xyxy: tensor([[355.9091,  77.0469, 526.3854, 236.8754]])\n",
      "xyxyn: tensor([[0.4923, 0.1598, 0.7281, 0.4914]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9237])\n",
      "data: tensor([[263.7568,  13.8819, 352.4370, 233.9216,   0.9237,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[308.0969, 123.9017,  88.6802, 220.0397]])\n",
      "xywhn: tensor([[0.4261, 0.2571, 0.1227, 0.4565]])\n",
      "xyxy: tensor([[263.7568,  13.8819, 352.4370, 233.9216]])\n",
      "xyxyn: tensor([[0.3648, 0.0288, 0.4875, 0.4853]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.9215])\n",
      "data: tensor([[450.3641, 261.1608, 528.6795, 454.1988,   0.9215,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[489.5218, 357.6798,  78.3154, 193.0381]])\n",
      "xywhn: tensor([[0.6771, 0.7421, 0.1083, 0.4005]])\n",
      "xyxy: tensor([[450.3641, 261.1608, 528.6795, 454.1988]])\n",
      "xyxyn: tensor([[0.6229, 0.5418, 0.7312, 0.9423]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.8913])\n",
      "data: tensor([[226.9321, 273.7496, 440.9710, 458.5880,   0.8913,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[333.9515, 366.1688, 214.0388, 184.8384]])\n",
      "xywhn: tensor([[0.4619, 0.7597, 0.2960, 0.3835]])\n",
      "xyxy: tensor([[226.9321, 273.7496, 440.9710, 458.5880]])\n",
      "xyxyn: tensor([[0.3139, 0.5679, 0.6099, 0.9514]])\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([0.])\n",
      "conf: tensor([0.3947])\n",
      "data: tensor([[ 37.6310,  82.8444, 251.9773, 241.8970,   0.3947,   0.0000]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (482, 723)\n",
      "shape: torch.Size([1, 6])\n",
      "xywh: tensor([[144.8042, 162.3707, 214.3462, 159.0526]])\n",
      "xywhn: tensor([[0.2003, 0.3369, 0.2965, 0.3300]])\n",
      "xyxy: tensor([[ 37.6310,  82.8444, 251.9773, 241.8970]])\n",
      "xyxyn: tensor([[0.0520, 0.1719, 0.3485, 0.5019]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = 'image.png'\n",
    "img = cv2.imread(img)\n",
    "results = model(img)\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Extract bounding boxes\n",
    "    for box in boxes:\n",
    "        print(box)\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get coordinates of the box\n",
    "        confidence = box.conf[0]  # Get confidence score\n",
    "        label = int(box.cls[0])  # Get the class label\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Add label and confidence score\n",
    "        cv2.putText(img, f'{model.names[label]} {confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "cv2.imwrite('output1.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time:  0.19430756568908691\n",
      "[1.2750852e+01 1.0473948e+01 2.8135237e+01 3.5534500e+01 7.1738388e-05]\n",
      "draw time:  0.0001125335693359375\n",
      "detection time:  0.2720773220062256\n",
      "Output image saved as output.jpg and write time is:  1726940441.3801677\n"
     ]
    }
   ],
   "source": [
    "!python3 inference.py --model yolov8n.onnx --img image.jpg --img_size 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 723\n",
      "(1, 5, 8400)\n",
      "[11 35 60 71  4 40 46 75]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "onnx_model = 'yolov8n.onnx'\n",
    "input_image = 'image.png'\n",
    "model = cv2.dnn.readNetFromONNX(onnx_model)\n",
    "\n",
    "# Read the input image\n",
    "original_image = cv2.imread(input_image)\n",
    "[height, width, _] = original_image.shape\n",
    "print(height, width)\n",
    "\n",
    "# Prepare a square image for inference\n",
    "length = max((height, width))\n",
    "image = np.zeros((length, length, 3), np.uint8)\n",
    "image[0:height, 0:width] = original_image\n",
    "\n",
    "# Calculate scale factor\n",
    "scale = length / 640\n",
    "\n",
    "# Preprocess the image and prepare blob for model\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor=1 / 255, size=(640, 640), swapRB=True)\n",
    "model.setInput(blob)\n",
    "\n",
    "# Perform inference\n",
    "outputs = model.forward()\n",
    "print(outputs.shape)    \n",
    "\n",
    "# Prepare output array\n",
    "outputs = np.array([cv2.transpose(outputs[0])])\n",
    "rows = outputs.shape[1]\n",
    "\n",
    "boxes = []\n",
    "scores = []\n",
    "class_ids = []\n",
    "\n",
    "# Iterate through output to collect bounding boxes, confidence scores, and class IDs\n",
    "for i in range(rows):\n",
    "    classes_scores = outputs[0][i][4:]\n",
    "    (minScore, maxScore, minClassLoc, (x, maxClassIndex)) = cv2.minMaxLoc(classes_scores)\n",
    "    if maxScore >= 0.25:\n",
    "        box = [\n",
    "            outputs[0][i][0] - (0.5 * outputs[0][i][2]),\n",
    "            outputs[0][i][1] - (0.5 * outputs[0][i][3]),\n",
    "            outputs[0][i][2],\n",
    "            outputs[0][i][3],\n",
    "        ]\n",
    "        boxes.append(box)\n",
    "        scores.append(maxScore)\n",
    "        class_ids.append(maxClassIndex)\n",
    "\n",
    "# Apply NMS (Non-maximum suppression)\n",
    "result_boxes = cv2.dnn.NMSBoxes(boxes, scores, 0.25, 0.45, 0.5)\n",
    "print(result_boxes)\n",
    "detections = []\n",
    "\n",
    "# Draw the bounding boxes\n",
    "for i in result_boxes:\n",
    "    box = boxes[i]\n",
    "    x = int(box[0] * length)\n",
    "    y = int(box[1] * length)\n",
    "    w = int(box[2] * length)\n",
    "    h = int(box[3] * length)\n",
    "    class_id = class_ids[i]\n",
    "    score = scores[i]\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(original_image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Add label and confidence score\n",
    "\n",
    "cv2.imwrite('output2.jpg', original_image)\n",
    "print(detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "271.44568 160.28482 75.45444 280.81732 0.9076861\n",
      "390.99774 201.49124 143.35855 193.4961 0.9299027\n",
      "271.01727 159.10193 78.1042 279.64343 0.91869575\n",
      "391.71582 200.99384 144.02579 195.27289 0.937726\n",
      "124.92833 205.88744 190.86374 189.12448 0.9232094\n",
      "391.37473 201.23267 145.69696 196.30177 0.9362658\n",
      "541.26013 204.48322 135.85938 191.45844 0.9062034\n",
      "125.39397 207.0405 190.62463 189.1801 0.9342593\n",
      "541.5865 203.6667 135.32642 192.55582 0.92351747\n",
      "541.69073 203.97559 135.65787 192.58366 0.92040634\n",
      "437.2132 469.05588 64.85556 251.33075 0.94387007\n",
      "116.46841 475.4741 146.20439 247.42206 0.95187664\n",
      "295.0421 482.11768 181.76395 235.08496 0.9406672\n",
      "294.5409 481.31653 183.44617 233.98743 0.91053677\n",
      "436.5722 467.8059 65.74353 252.18277 0.9571278\n",
      "545.7831 480.35815 127.895325 233.35669 0.91712964\n",
      "116.859375 474.53738 145.96951 244.13983 0.934174\n",
      "295.3368 480.51147 180.86658 234.81946 0.9106795\n",
      "295.29956 480.1389 182.37378 233.96698 0.93204004\n",
      "294.88556 480.06958 182.852 233.8021 0.959304\n",
      "436.6814 470.27734 66.354645 248.54321 0.9133967\n",
      "546.48553 478.53583 128.05603 234.24158 0.90790737\n",
      "116.69873 474.89594 146.67615 245.40857 0.9164183\n",
      "116.84864 474.3291 147.523 244.44748 0.9755453\n",
      "295.59143 479.80823 180.13391 234.94592 0.9467997\n",
      "295.6433 479.67783 181.45906 233.17487 0.9524659\n",
      "295.52063 478.80457 183.01575 234.21387 0.95852673\n",
      "545.96576 478.1801 129.48413 232.93103 0.9080999\n",
      "546.0268 478.05396 130.0098 231.59973 0.9271612\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes = output[0][0].T  # Depending on how the output is structured, you might need further post-processing\n",
    "\n",
    "# Extract and draw bounding boxes (assuming output[0] contains bounding boxes)\n",
    "boxes = boxes[boxes[:, 4] > 0.9]  # Confidence threshold\n",
    "print(len(boxes))\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2, confidence = box[:5]\n",
    "    print(x1, y1, x2, y2, confidence)\n",
    "    \n",
    "    # Scale the coordinates back to original image size\n",
    "    x1 = int(x1 * img.shape[1] / 640)\n",
    "    y1 = int(y1 * img.shape[0] / 640)\n",
    "    x2 = int(x2 * img.shape[1] / 640)\n",
    "    y2 = int(y2 * img.shape[0] / 640)\n",
    "    \n",
    "    # Draw bounding box and label on the image\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    cv2.putText(img, f'{confidence:.2f}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36,255,12), 2)\n",
    "\n",
    "# Save the output image\n",
    "cv2.imwrite('output_from_onnx.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.98 ðŸš€ Python-3.12.3 torch-2.4.1+cu121 CPU (AMD Ryzen 5 PRO 3400G with Radeon Vega Graphics)\n",
      "YOLOv5n summary (fused): 193 layers, 2,503,139 parameters, 0 gradients, 7.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov5.pt' with input shape (1, 3, 416, 416) BCHW and output shape(s) (1, 5, 3549) (5.0 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.16.2 opset 12...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.5s, saved as 'yolov5.onnx' (9.7 MB)\n",
      "\n",
      "Export complete (0.9s)\n",
      "Results saved to \u001b[1m/home/taras-desktop/Documents/ucu/embeddedAI/project/model_training\u001b[0m\n",
      "Predict:         yolo predict task=detect model=yolov5.onnx imgsz=416  \n",
      "Validate:        yolo val task=detect model=yolov5.onnx imgsz=416 data=/content/drive/MyDrive/Tree Decetion.v2-trees-v2.yolov5pytorch/data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov5.onnx'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = ultralytics.YOLO('yolov5.pt')\n",
    "model5.export(format='onnx', opset=12)  # You can adjust the opset if needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
